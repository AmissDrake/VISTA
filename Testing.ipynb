{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd6a8d8-0416-4613-9768-4fd5213e6dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "import time\n",
    "import pyautogui\n",
    "from collections import deque\n",
    "\n",
    "def normalize_lm(flat):\n",
    "    lm = flat.reshape(21, 3)\n",
    "    wrist = lm[0].copy()\n",
    "    lm -= wrist\n",
    "    norm = np.linalg.norm(lm)\n",
    "    return (lm / norm if norm > 0 else lm).flatten()\n",
    "\n",
    "def normalize_dual_lm(flat):\n",
    "    lm1 = flat[:63].reshape(21, 3)\n",
    "    lm2 = flat[63:].reshape(21, 3)\n",
    "    wrist1 = lm1[0].copy()\n",
    "    wrist2 = lm2[0].copy()\n",
    "    lm1 -= wrist1\n",
    "    lm2 -= wrist2\n",
    "    norm1 = np.linalg.norm(lm1)\n",
    "    norm2 = np.linalg.norm(lm2)\n",
    "    norm1 = norm1 if norm1 > 0 else 1\n",
    "    norm2 = norm2 if norm2 > 0 else 1\n",
    "    norm1_lm = lm1 / norm1\n",
    "    norm2_lm = lm2 / norm2\n",
    "    return np.concatenate([norm1_lm.flatten(), norm2_lm.flatten()])\n",
    "\n",
    "# Load models\n",
    "left_clf = joblib.load('left_hand_model.pkl')\n",
    "right_clf = joblib.load('right_hand_model.pkl')\n",
    "dual_clf = joblib.load('dual_hand_model.pkl')\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,\n",
    "                       min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Starting hand-gesture keyboard. Press 'q' to quit.\")\n",
    "\n",
    "typed_text = \"\"\n",
    "last_prediction = \"\"\n",
    "last_time = time.time()\n",
    "delay = 2.0  # delay before accepting a new gesture to avoid rapid repeats\n",
    "\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "\n",
    "    current_time = time.time()\n",
    "    gesture_text = \"\"\n",
    "    confidence = 0\n",
    "    pred = \"\"\n",
    "\n",
    "    if res.multi_hand_landmarks and res.multi_handedness:\n",
    "        # Map hands detected by label, even if 1 or 2 hands\n",
    "        hands_detected = {'Left': None, 'Right': None}\n",
    "        for i, handedness in enumerate(res.multi_handedness):\n",
    "            label = handedness.classification[0].label\n",
    "            hands_detected[label] = res.multi_hand_landmarks[i]\n",
    "\n",
    "        # If both hands detected, use dual clf no matter their position (fix requested)\n",
    "        if hands_detected['Left'] is not None and hands_detected['Right'] is not None:\n",
    "            lm_left = hands_detected['Left']\n",
    "            lm_right = hands_detected['Right']\n",
    "\n",
    "            coords_left = np.array([[p.x, p.y, p.z] for p in lm_left.landmark]).flatten()\n",
    "            coords_right = np.array([[p.x, p.y, p.z] for p in lm_right.landmark]).flatten()\n",
    "            combined = np.concatenate([coords_left, coords_right])\n",
    "            norm = normalize_dual_lm(combined)\n",
    "\n",
    "            pred = dual_clf.predict([norm])[0]\n",
    "            confidence = np.max(dual_clf.predict_proba([norm])) if hasattr(dual_clf, 'predict_proba') else 1.0\n",
    "            gesture_text = f\"Dual: '{pred}' ({confidence*100:.1f}%)\"\n",
    "\n",
    "            mp_drawing.draw_landmarks(frame, lm_left, mp_hands.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(frame, lm_right, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        else:\n",
    "            # Only one hand detected: figure out which one, predict accordingly\n",
    "            # Also check if left or right hand is present and get landmarks\n",
    "            for side in ['Left', 'Right']:\n",
    "                lm = hands_detected[side]\n",
    "                if lm is not None:\n",
    "                    flat = np.array([[p.x, p.y, p.z] for p in lm.landmark]).flatten()\n",
    "                    norm = normalize_lm(flat)\n",
    "\n",
    "                    clf = left_clf if side == 'Left' else right_clf\n",
    "                    pred = clf.predict([norm])[0]\n",
    "                    confidence = np.max(clf.predict_proba([norm])) if hasattr(clf, 'predict_proba') else 1.0\n",
    "                    gesture_text = f\"{side}: '{pred}' ({confidence*100:.1f}%)\"\n",
    "\n",
    "                    mp_drawing.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)\n",
    "                    break\n",
    "\n",
    "    # --- Type only if prediction changed AND delay passed ---\n",
    "    if pred:\n",
    "        if pred != last_prediction:\n",
    "            # New gesture, wait delay seconds to confirm\n",
    "            last_time = current_time\n",
    "            last_prediction = pred\n",
    "        else:\n",
    "            # Same gesture: type only if delay passed since last typing\n",
    "            if (current_time - last_time) >= delay:\n",
    "                last_time = current_time\n",
    "                # Perform action\n",
    "                if pred == 'left_click':\n",
    "                    pyautogui.click(button='left')\n",
    "                elif pred == 'right_click':\n",
    "                    pyautogui.click(button='right')\n",
    "                elif pred == 'backspace':\n",
    "                    pyautogui.press('backspace')\n",
    "                    typed_text = typed_text[:-1] if typed_text else ''\n",
    "                elif pred == 'space':\n",
    "                    pyautogui.press('space')\n",
    "                    typed_text += ' '\n",
    "                elif pred == 'enter':\n",
    "                    pyautogui.press('enter')\n",
    "                    typed_text += '\\n'\n",
    "                elif pred in [',', '.', '?']:\n",
    "                    pyautogui.press(pred)\n",
    "                    typed_text += pred\n",
    "                else:\n",
    "                    pyautogui.write(str(pred))\n",
    "                    typed_text += str(pred)\n",
    "\n",
    "    # --- UI Overlay ---\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (0, 0), (frame.shape[1], 100), (20, 20, 20), -1)\n",
    "    alpha = 0.7\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "    cv2.putText(frame, f\"Typed: {typed_text[-50:]}\", (10, 65),\n",
    "                font, 1.1, (255, 255, 255), 2)\n",
    "    if gesture_text:\n",
    "        cv2.putText(frame, gesture_text, (10, 30),\n",
    "                    font, 0.8, (100, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Hand Gesture Keyboard\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
